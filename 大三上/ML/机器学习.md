# 机器学习

## AlphaGo

博弈树：有向树

博弈树搜索：具有完全信息的二人零和有限确定博弈

* 极小极大算法
  * 在所有极大值中找极小值
  * 有max（自己）和min（对方），我们已知最后的回报，但不知道中间的回报
  * 由于每个人都追求自己的利益最大化，则在min层对手会选择最小化下一步的得分，所以选择确定；max层会最大化下一步的得分，选择也确定，所以最终所有选择都会确定
  * 类似于反推
  * 问题：太多
* alpha-beta剪枝
  * min：alpha剪枝，把大的减去（肯定不会选这一支）
  * max：beta剪枝，把小的减去（也肯定不会选）
  * alpha为最大下界（初始化-无穷），beta为最小上界（初始化+无穷）
  * 对所有节点，实时更新alpha和beta值，并可以通过上层进行值的传递
  * 例如：min层有两个节点，其中一个下接3 17，一个下接2 xx xxx，那么在判断完3 17选择3后，beta = 3，上层节点和另外一个节点的alpha更新为3（现在应有的下界），那么当查找2之后，显然这个节点再怎么样选择最后max层也不会选它，所以剪枝（max层也是如此，上面是一个min层，如果一个max层的节点值足够小，那可能比这个节点还大的节点都没有必要去遍历了）
* 蒙特卡洛搜索：基于随机方法的查找，每个节点值为 赢次数/访问次数
  * 选择：节点分三类
    * 未访问：该节点未走到
    * 未完全展开：子节点还可以走
    * 完全展开：子节点完全访问
    * 有一定选择策略
  * 扩展：对选择到的节点，如果还没有完全扩展（还有其他可能性），便扩展一种或多种新的可能性
  * 模拟：从扩展到的节点的状态开始，根据某种策略进行模拟游戏，得到输赢（此处并不在下面拓展选择树，只是单纯的模拟，直到terminal）
  * 反向传播：将本次输赢情况和访问情况按照选择路径反向传播
  * 然而我们必须了解，如果随机选择，这个算法并不完善
    * 提出UCB算法（置信区间上限）
  * $v_i + C\sqrt{\frac{\ln(N)}{n_i}}$，其中vi是该节点获取value预计（Qi/ni），N是树节点visit次数，ni是子节点visit次数，取最大节点



策略和价值网络：

* 为减少搜索树的分支因子和深度
* 策略网络搜索更好的行动
* 价值网络估计树的下行分枝
* 方法：
  * 监督学习 + 强化学习 -> 策略网络
  * 强化学习：给定宏观状态，估计胜利概率 -> 价值网络



## 增强学习

### Q学习

以回报为核心的通用算法：总是给予回报最大选择合适的动作

* 使用未来回报更新当前回报
  * Q(state, action) = R(state, action) + gamma·Max(Q(next state, all actions))
* 相当于迭代过程



算法：

* 输入：状态/动作 - 回报矩阵R，目标状态GOAL
* 输出：从任何初始状态导目标状态的最小路径Q
  * 设置学习率gamma，R
  * Q = 0
  * 进行循环：
    * 随机选择初始状态
    * 如果不是目标状态：
      * 选择所有可能的动作，测试：
        * 选择Q值最大的状态进行Q值更新
        * Q(state, action) = R(state, action) + gamma·Max(Q(next state, all actions))
    * 若Q值稳定，结束循环，否则继续迭代



## 不等式

### Markov不等式

$$
P(X > t) \le \frac{E(X)}{t}
$$

表示过于偏离某值的时候的概率会比较小



### Chebyshev不等式

$$
P(|X - \mu| \ge t) \le \frac{\sigma^2}{t^2}
$$

邻域与方差有关



### Hoeffding不等式

$$
X_1, \dots, X_2 ~~~ i.i.d. \newline
E(X_i) = 0, a_i \le X_i \le b_i, let ~ \epsilon > 0, any ~ t>0 \newline
P(\sum_{i=1}^{n} X_i \ge \epsilon) \le e^{-t\epsilon} \Pi_{i=1}^{n} e^{t^2 \frac{(b_i-a_i)^2}{8}}
$$

### Cauchy-Schwarz不等式

$$
E(|XY|) \le \sqrt{E(X^2)E(Y^2)}
$$

### Jensen不等式

如果g是凸的（下凸）
$$
g(X) \ge g(E(X))
$$


## The PAC Framework

> Probably approxiamately correct

Setting: Inductively learning an unknown target function, given training examples and a hypothesis space



Desirable:

* Complexity of hypo space
* Acc of approximation
* Probability of outputting a successful hypo
* How the training examples are presented



Training error: 计算的方程 与 预期方程间，不相等的元素 之和 除n（模拟在取一个x的时候会有的错误概率）

True error: 空间D上存在一个分布，在服从此分布下的x使得预测与预期不相等的概率

* 不可观测

